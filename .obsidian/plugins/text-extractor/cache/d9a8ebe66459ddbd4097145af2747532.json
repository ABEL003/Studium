{"path":"1.Semester/Mathematik Aalen/Nachschlagwerke/Formelsammlung.pdf","text":"STATISTIK I und II PD Dr. Stadnitski 1 FORMELSAMMLUNG Deskriptive Statistik Relative Häufigkeit [h(aj)=absolute Häufigkeit, Anzahl] n=Stichprobengröße, k=Anzahl verschiedener Merkmalsausprägungen p-Quantile Momente 1 1 n kk i i mx n     1 1 n kk zentral i i mx x n   Arithmetisches Mittel n ist Stichprobengröße= Anzahl der Beobachtungen k ist Anzahl verschiedener Merkmalsausprägungen Empirische Varianz (Streuung) Unverzerrte Varianz Standardabweichung 2ss Variationskoeffizient /Vs x Tschebyscheffsche Ungleichung  2 2() s PX x c c   wenn cks , erhält man 2 1 ()PX x k s k    Schiefe und Exzess 34 34 Ex 3 zentral zentralmm Sch ss   z-Transformation i i x x z s   Ausreißer Werte außerhalb des Intervalls [Q1-1,5IQB; Q3+1,5IQB] n )a(h )a(f j j     k 1j j 1)a(f  222 2 11 11 nn ii ii sx x x n x nn        )a(fxas j k 1j 2 j 2    222 1 22 1 xxxx n s n i i          p)n(k Zahlganze pn falls, Zahl)ganze folgende pn aufist (k Zahlganze keine pn falls , )( 2 1: 1         kk k p xx x x    k 1j jj )a(fax 1 1 n i i x x n     222 1 1 11 n ui i n sx x s nn    221 s u n s n   STATISTIK I und II PD Dr. Stadnitski 2 Odds Ratio 11 12 21 22 / / hh hh   hij= absolute Häufigkeit in der Zelle ij Produkt-Moment Korrelation (Pearson) Kovarianz Rangkorrelations- koeffizient von Spearman R(xi) und R(yi) sind Rangzahlen -Koeffizient (2x2 Kontingenztabelle) n =Stichprobengröße, k= Zeilenanzahl, l= Spaltenanzahl Kontingenzkoeffizient (kxl Kontingenztabelle) Regression Y=a+bX, Y ist abhängige Variable (AV)=Kriterium, X ist unabhängige Variable (UV)=Prädiktor Standardisierte einfache Regression: Yz=rXz Lineare Transformation Für ii bxay  gilt xbay  , )X(Varb)y(Var 2 a und b sind Konstanten (feste Zahlen)        n 1i n 1i 2 i 2 i n 1i ii XY )yy()xx( )yy)(xx( r YX XY yX XY ss s ss yx r  22 ),cov( )1n(n d6 1r 2 n 1i 2 i S     )y(R)x(Rd iii  n 2      k 1i l 1j )j,i(e 2 )j,i(e)j,i(b2 h )hh( 2 2 n K    )l,kmin( 1)l,kmin( K max   n )yy)(xx( )y,xcov( n 1i ii    yxxysyx xy ),cov( max * K K K  xbya         n 1i 2 i n 1i ii )xx( )yy)(xx( b      22 i ii xnx yxnyx b x y xy X s s r s yx b  2 ),cov( anzGesamtvari Varianz erklärte )( )ˆ( 1 )( )ˆ( 1 2 1 2 1 2 1 2 2                n i i n i i n i i n i i yy yy yy yy R STATISTIK I und II PD Dr. Stadnitski 3 Wahrscheinlichkeitsrechnung Bedingte Wahrscheinlichkeit Additionstheorem Multiplikationstheorem Satz von der totalen Wahrscheinlichkeit k= Anzahl disjunkter Teilmengen (Ereignisse) Theorem von Bayes Unabhängigkeit P(BA)=P(B) P(AB)=P(A) P(AB)=P(A)*B(B) Bestimmung von Wahrscheinlichkeiten  P(X b)=F(b)  P(a<X b)=F(b)-F(a)  P(X>a)=1-F(a)  P(a X  b)=F(b)-F(a)+P(X=a) [F(x)- Verteilungsfunktion des Merkmals/ der Zufallsvariablen X] Kombinatorik N= Populationsgröße, n= Stichprobengröße Zurücklegen (Wiederholung) ja nein Variation (Reihenfolge berücksichtigt) Kombination (Reihenfolge nicht berücksichtigt) Permutationen N! )BA(p)B(p)A(p)BA(p  )BA(p)B(p)AB(p)A(p)BA(p  )AB(p)A(p)B(p i k 1i i   )AB(P )B(P )A(P )BA(P         k 1i ii iii i )A(P)AB(P )A(P)AB(P )B(P )BA(P )BA(P nN )1nN(...)1N(N )!nN( !N          n 1nN !n!...n !N k1 !n)!nN( !N n N        )A(p )BA(p )AB(p   STATISTIK I und II PD Dr. Stadnitski 4 Zufallsvariablen und Verteilungen Häufigkeits- verteilung WAHRSCHEINLICHKEITSVERTEILUNG Merkmal X Zufallsvariable X diskret stetig {a1, ..., ak} k verschiedener Ausprägungen {x1, ..., xk} Wertebereich, Ergebnismenge, Träger Werterbereich ist Teilmenge aus mit = Menge der reellen Zahlen f(aj) relative Häufigkeit von aj WAHRSCHEINLICHKEITSFUNKTION p(xi)    k i ki xpxpxp 1 1 1)(..)()( DICHTEFUNKTION f(x)     1)( dxxf Kumulierte Häufigkeitsverteilung    xa j j afxF )()( VERTEILUNGSFUNKTION Arithmetisches Mittel    k J jj afax 1 )( ERWARTUNGSWERT VARIANZ KOVARIANZ   ij ijyjxiXY pyx ))((        dxdyyxfyx yjxiXY ),())((  •Cov(X,Y)=E(XY)-E(X)E(Y) •Cov(X+a,Y+b)=Cov(X,Y) •Cov(aX, bY)=abCov(X,Y) •Cov(X, Y+Z)=Cov(X,Y)+Cov(X,Z) •Cov(X, Y)=Cov(Y,X) •Cov(X, X)=Var(X) •Sind X, Y unabhängig Cov(X,Y)=0    k j jaf 1 1)(   )()()(),( YEXEXYEYXEYXCov YXXY      xx i i xpxF )()(    x dttfxF )()(  i ii xpxXE )()(      dxxxfXE )()(    i ii xpxXVar )()()( 22      dxxfxXVar )()()( 22  Summenvariablen Y=aX1+bX2 E(Y)=aE(X1)+bE(X2) Var(Y)=a 2Var(X1)+b 2Var(X2)+2abCov(X1,X2) STATISTIK I und II PD Dr. Stadnitski 5 Diskrete Verteilungen Verteilung P(X=x) Erwartungswert Varianz X ~B(n,p) Binomial n= Stichprobengröße p= WS für „Erfolg“ np np(1-p) X ~H(n, N, M) Hypergeometrisch N= Populationsgröße M= Anzahl der „Erfolge“ M/N=WS für „Erfolg“ n= Stichprobengröße X~Po() Poisson   Stetige Verteilungen Verteilung Erwartungswert Varianz Gleichverteilung (a+b)/2 a= kleinster Wert, b= größter Wert (b-a) 2/12 Normalverteilung  2 Standardnormalverteilung 0 1 2(df)-Verteilung df 2df t (df)-Verteilung 0 df/(df-2) für df>2 F(n, m)-Verteilung n= Zähler df m= Nenner df df= degrees of freedom, Freiheitsgrade m/(m-2) (für m2) (für m4) Ermitteln von nicht-tabellierten F-Werten: xnx )p1(p x n )xX(P                             n N xn MN x M )xX(P N M n  1N nN N M 1 N M n            e !x )xX(P x 2 2 )2)(4( )2(2   mmn nmm 1 1 (, ) (, ) Fm n Fn m    STATISTIK I und II PD Dr. Stadnitski 6 Intervallschätzung µ Normalverteilte Grundgesamtheit,  bekannt z sind Quantile der Standardnormalverteilung N(0,1) µ Normalverteilte Grundgesamtheit,  unbekannt t sind Quantile der t(n-1)-Verteilung, n ist Srtichprobenröße µ beliebig verteilte Grundgesamtheit, n>30 µ dichotome Grundgesamtheit    n 1i i 5nx5 (µ=p= Wahrscheinlichkeit für „Erfolg“)  )(ix =absolute Häufigkeit der „Erfolge“ n x x n i i  1 = relative Häufigkeit der Erfolge 2 Normalverteilte Grundgesamtheit c1 und c2 sind /2- bzw. 1-/2-Quantile der 2(n-1)Verteilung  Bivariate Normalverteilung            n zx; n zx )2/1()2/1(         n s tx n s tx uu )2/1()2/1( ;  1 )( 1 2      n xx s n i i u              n )x1(x zx; n )x1(x zx )2/1()2/1(          1 2 2 2 )1( ; )1( c sn c sn uu         n s zx n s zx uu )2/1()2/1( ;            2-n R-1 zR; 2-n R-1 zR 2 α/21 2 α/21 STATISTIK I und II PD Dr. Stadnitski 7 Parametrische Tests TEST TESTSTATISTIK VERTEILUNG DER TESTSTATISTIK UNTER H0 z-Test (Gauß-Test) H0: µ=µ0 N(0,1) Einstichproben t-Test H0: µ=µ0 t(n-1) 2-Test für Varianz H0: =0 2 0 2 1 2 2 0 )1( )( 1  u n i i Sn XX    2(n-1) t-Test für zwei abhängige Stichproben (Differenztest) H0: µd=0 mit iii yxd  t(n-1) ab n>30 approximativ N(0,1) t-Test für zwei unabhängige Stichproben H0: µ1=µ2 1 und 2 bekannt N(0,1) t-Test für zwei unabhängige Stichproben H0: µ1=µ2  unbekannt, 1= 2 t(n1+n2-2) Approximativ N(0,1) für n1>30 und n2>30 Zweistichproben F-Test (Test auf Varianzenhomogenität) H0: 2 1=2 2 F (n1-1, n2-1) n/ X 0   nS X u / 0 ns d nn dd d d n i i / )1( )( 1 2     )XX( 21 21 XX               21 2 2 2 2 1 2 1 )XX( n 1 n 1 nn 21 21 21 11~ ˆmit ˆ 21 21 nn S XX XX XX       2 2 2 1 u u S S 2 )1()1(~ 21 2 22 2 11    nn SnSn S uu STATISTIK I und II PD Dr. Stadnitski 8 Korrelationstest H0: =0 Für 0=0 mit YX XY SS S R  : Für 00 und n>25: t~(n-2) Approximativ N(0,1) Vergleich zweier Korrelationskoeffizienten H0: 1=2            1 1 ln5,0Z (Fisher Transformstion) Approximativ N(0,1) Signifikanztest für einfache Kontraste in einer ANOVA Kontrast Ein Kontrast ist eine gewichtete Summe der Gruppenmittelwerte: K=c1M1+c2M2+…+ckMk=ciMi Teststatistik K K V~t(n k) ˆσ  Standardfehler des Kontrastes K 2kk k k 22 i ii i i i i i i1 i1 i 1 i1 i σVar(K) σ Var(K) Var c M Var(c M ) c Var(M ) c n           Schätzung der Standardfehler bei Varianzenhomogenität: 1 2=… =k 2 =2: 22 2kk k 22 2ii iK i1 i1 i1ii i σc c ˆˆVar(K) c σ σ σ nn n     Schätzung der Merkmalsvarianz (Varianz Innerhalb) aus den Streuungen aller k Gruppen 22 222 u1 1 uk k I 1k S (n 1) ...S (n 1) ˆσS S n ...n k       ) )2/(1 2 2  nR R 3 1 1 ln 1 1 ln5,0 0 0            n R R   3n 1 3n 1 mit ZZ 21 )ZZ( )ZZ( 21 21 21         STATISTIK I und II PD Dr. Stadnitski 9 Nichtparametrische Tests TEST TESTSTATISTIK VERTEILUNG DER TESTSTATISTIK UNTER H0 2-Anpassungstest (Goodness of Fit) H0: F=F0 bzw. H0: 2=0 he(j)5 für alle j j=1,...,k 2(k-1) 2- Unabhängigkeitstest H0: 2=0 k Ausprägungen des 1. Merkmals l Ausprägungen des 2. Merkmals  Erwartete Häufigkeiten bekannt: 2(kl-1)  Erwartete Häufigkeiten unbekannt (= aus den Daten geschätzt): 2[(k-1)(l-1)] McNemar-Test 22 e(j) b(j)2 () j=1 e( j j) eh5, nur \"Veränderungen\" betracht t (h -h ) h e =,    2(1) U-Test (zwei unabhängige Stichproben) T= Rangsumme, n= Stichprobengröße Mit R =Durchschnittsrang Approximativ N(0,1) Wilcoxon-Test (zwei abhängige Stichproben) T- Rangsumme der Differenzen mit dem selteneren Vorzeichen T´- Rangsumme der Differenzen mit dem häufigeren Vorzeichen Kritische Werte sind tabelliert (Vgl. S.18) Ab n>25 approximativ N(0,1)     k 1j 2 )j(e )j(b)j(e2 h )hh(     k 1i l 1j )j,i(e 2 )j,i(e)j,i(b2 h )hh( 12 )1nn(nn 2121 U   u UU   1 11 21 T 2 )1n(n nnU    2 nn bzw. RR:H 21 u210   T T T   24 )1n2)(1n(n T   4 )1n(n µ btw. ´TT:H T0       k 1i l 1j )j,i(e 2 )j,i(e)j,i(b2 h )hh( STATISTIK I und II PD Dr. Stadnitski 10 Regressionsanalyse H0: 1 = 2 =... p = 0 H0: j = b H0: 1-2 =c  12 12 12 ˆˆ 11 1 2β-β ˆˆβ-β ˆˆβ-β -c ˆˆ ˆ ˆˆV= ~t(n-p-1) mit σ = Var(β )+Var(β )-2Cov(β ,β ) ˆσ LR-Test   M0 M1 M1 0 RQS -RQS /m ~F(m,n-p-1) RQS / n-p-1 RQS=Residualquadratsumme; m=Anzahl der Restriktionen unter H Einfaktorielle ANOVA mit k Stufen (between subject) Quelle der Varianz Quadratsumme (QS) df Mittel der Quadrate Testgröße, Verteilung unter H0 TREATMENT (zwischen den Gruppen) k-1 QS/df FEHLER (innerhalb der Gruppen) n-k TOTAL n-1 Post-Hoc Vergleiche mit Bonferroni-Korrektur: kor.=/m , pkor.=pzw.*m mit m=Anzahl verschiedener Paarvergleiche 2 k   . Effektstärkemaße Cohens d EG KGd      Glass Delta EG KG KG      Aufgeklärte Varianz, f 22 Modell Gesamt QS R=η = QS , 2 21 R f R    2 2 R/p V= ~F(p, n-p-1) mit n=Stichprobengröße; p=Anzahl der Prädiktoren 1-R / n-p-1 j jj 2 2 ,j 2 1 ˆ22β ˆ Xβ ˆˆβ-b ˆσˆ ˆV= ~t(n-p-1) mit , σ = , , ˆσnS 1 ˆu=Modellresiduen j j n i Yx i j x uS Sn p        k i n j ij i XX 11 2    k i n j jij i XX 11 2    k 1i n 1j 2 ij i XX );1(~ knkF MQ MQ Innerhalb Zwischen  STATISTIK I und II PD Dr. Stadnitski 11 Einfaktorielle ANOVA mit Messwiederholung (within subject) Quelle der Varianz Quadratsumme (QS) df Mittel der Quadrate Testgröße, Verteilung unter H0 ZWISCHEN VPN n-1 QS/df INNERHALB VPN n(k-1) TREATMENT (zwischen den Gruppen) k-1 RESIDUAL (n-1)(k- 1) GESAMT kn-1    n i i XXk 1 2 .    n i k j iij XX 11 2 .    k j j XXn 1 2 .    n i k j jiij XXXX 11 2 ..    n i k j ij XX 11 2 Treatment Residual MQ ~ MQ F[k-1;(n-1)(k-1)] 12 Binomialverteilungen 13 Verteilungsfunktion der Standardnormalverteilung 14 Verteilungsfunktion der 2-Verteilungen 15 Verteilungsfunktion der 2-Verteilungen 16 Verteilungsfunktionen der t-Verteilungen 17 0.95-Quantile der F-Verteilung mit (n,m) Freiheitsgraden 18 0.99-Quantile der F-Verteilung mit (n,m) Freiheitsgraden 19 Kritische Werte für den Wilcoxon-Test n p=0,05 p=0,01 p=0,001 6 0 7 2 8 3 0 9 5 1 10 8 3 11 10 5 0 12 13 7 1 13 17 9 2 14 21 12 4 15 25 15 6 16 29 16 8 17 34 23 11 18 40 27 14 19 46 32 18 20 52 37 21 21 58 42 25 22 65 48 30 23 73 54 35 24 81 61 40 25 89 68 45","libVersion":"0.3.1","langs":""}